{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f759408a-009e-43ab-9f9b-d52cbdacbcf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
      "Best C: 100\n",
      "Validation log-loss: 0.22143236893686868\n",
      "Saved submission_improved.csv\n"
     ]
    }
   ],
   "source": [
    "# 0.imports\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\n",
    "\n",
    "# 1. Helper to read \"id|--|text\" files into a DataFrame\n",
    "def load_two_col_txt(path, col_names):\n",
    "    records=[]\n",
    "    with open(path, encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        for line in f:\n",
    "            if \"|--|\" in line:\n",
    "                pid, text = line.rstrip(\"\\n\").split(\"|--|\", 1)\n",
    "            else:\n",
    "                pid, text = None, line.rstrip(\"\\n\")\n",
    "            records.append([pid, text])\n",
    "    return pd.DataFrame(records, columns=col_names)\n",
    "\n",
    "# 2.  Load input files\n",
    "\n",
    "DATA_DIR = \"C:/Users/gkara/Desktop/5th year/summer/epeksergasia glwssas/data_new/\"\n",
    "\n",
    "#load\n",
    "abstracts = load_two_col_txt(os.path.join(DATA_DIR, \"abstracts.txt\"),\n",
    "                             [\"paper_id\",\"abstract\"])\n",
    "authors   = load_two_col_txt(os.path.join(DATA_DIR, \"authors.txt\"),\n",
    "                             [\"paper_id\",\"authors\"])\n",
    "edgelist  = pd.read_csv(os.path.join(DATA_DIR, \"edgelist.txt\"),\n",
    "                       sep=\",\", names=[\"src\",\"dst\"], dtype=str, header=None)\n",
    "test_pairs= pd.read_csv(os.path.join(DATA_DIR, \"test.txt\"),\n",
    "                       sep=\",\", names=[\"src\",\"dst\"], dtype=str, header=None)\n",
    "\n",
    "#clean whitespace from all paper IDs\n",
    "\n",
    "for df in (abstracts, authors):\n",
    "    df[\"paper_id\"] = df[\"paper_id\"].str.strip()\n",
    "edgelist[\"src\"] = edgelist[\"src\"].str.strip()\n",
    "edgelist[\"dst\"] = edgelist[\"dst\"].str.strip()\n",
    "test_pairs[\"src\"] = test_pairs[\"src\"].str.strip()\n",
    "test_pairs[\"dst\"] = test_pairs[\"dst\"].str.strip()\n",
    "\n",
    "# 3.  Build positive & negative samples\n",
    "\n",
    "# Mark all existing citations as positive examples\n",
    "edgelist[\"label\"] = 1\n",
    "nodes = pd.unique(edgelist[[\"src\",\"dst\"]].values.ravel())\n",
    "\n",
    "# Set negative sampling ratio\n",
    "NEG_RATIO = 0.2\n",
    "n_pos = len(edgelist)\n",
    "n_neg = int(n_pos * NEG_RATIO)\n",
    "\n",
    "# Sample random pairs of nodes as negative examples\n",
    "neg = pd.DataFrame({\n",
    "    \"src\": np.random.choice(nodes, size=n_neg, replace=True),\n",
    "    \"dst\": np.random.choice(nodes, size=n_neg, replace=True)\n",
    "})\n",
    "neg = neg[~neg.apply(tuple,1).isin(edgelist.apply(tuple,1))].head(n_neg)\n",
    "neg[\"label\"] = 0\n",
    "\n",
    "samples = pd.concat([edgelist, neg], ignore_index=True)\n",
    "\n",
    "# 4. TF-IDF + cosine feature\n",
    "tfv = TfidfVectorizer(max_features=10000, stop_words=\"english\")\n",
    "tfv.fit(abstracts[\"abstract\"].fillna(\"\"))\n",
    "\n",
    "# Transform all abstracts into TFâ€“IDF matrix\n",
    "A = tfv.transform(abstracts[\"abstract\"].fillna(\"\"))\n",
    "id2idx = {pid:i for i,pid in enumerate(abstracts[\"paper_id\"])}\n",
    "\n",
    "def pair_cosine_batch(df):\n",
    "    idx_u = df[\"src\"].map(id2idx).fillna(-1).astype(int).values\n",
    "    idx_v = df[\"dst\"].map(id2idx).fillna(-1).astype(int).values\n",
    "    valid = (idx_u>=0)&(idx_v>=0)\n",
    "    sims = np.zeros(len(df), dtype=np.float32)\n",
    "    Au = A[idx_u[valid]];   Av = A[idx_v[valid]]\n",
    "    sims_valid = (Au.multiply(Av)).sum(axis=1).A1\n",
    "    sims[valid] = sims_valid\n",
    "    return sims.reshape(-1,1)\n",
    "\n",
    "X_cos = pair_cosine_batch(samples)\n",
    "\n",
    "# 5. Author-Jaccard\n",
    "\n",
    "# Build mapping from paper_id to authors string\n",
    "auth_map = dict(zip(authors[\"paper_id\"], authors[\"authors\"]))\n",
    "au_src_list = samples[\"src\"].map(auth_map).fillna(\"\").values\n",
    "au_dst_list = samples[\"dst\"].map(auth_map).fillna(\"\").values\n",
    "\n",
    "def jaccard_auth(a, b):\n",
    "    sa = set(a.split(\",\")) if a else set()\n",
    "    sb = set(b.split(\",\")) if b else set()\n",
    "    if not sa and not sb:\n",
    "        return 0.0\n",
    "    return len(sa & sb) / len(sa | sb)\n",
    "\n",
    "author_jac = np.array([jaccard_auth(a,b) for a,b in zip(au_src_list, au_dst_list)]).reshape(-1,1)\n",
    "\n",
    "# 6.  Compute Common Neighbors feature\n",
    "G = nx.DiGraph()\n",
    "G.add_edges_from(edgelist[[\"src\",\"dst\"]].values)\n",
    "Gu = G.to_undirected()\n",
    "\n",
    "cn = []\n",
    "for u,v in zip(samples[\"src\"], samples[\"dst\"]):\n",
    "    try:\n",
    "        cn.append(len(list(nx.common_neighbors(Gu,u,v))))\n",
    "    except:\n",
    "        cn.append(0)\n",
    "X_common = np.array(cn).reshape(-1,1)\n",
    "\n",
    "# 7.  Assemble feature matrix and labels\n",
    "\n",
    "X_all = np.hstack([X_cos, author_jac, X_common])\n",
    "y = samples[\"label\"].astype(int).values\n",
    "\n",
    "# 8. Split + scale\n",
    "\n",
    "X_tr, X_va, y_tr, y_va = train_test_split(\n",
    "    X_all, y, stratify=y, test_size=0.2, random_state=42\n",
    ")\n",
    "scaler = StandardScaler(with_mean=False)\n",
    "X_tr = scaler.fit_transform(X_tr)\n",
    "X_va = scaler.transform(X_va)\n",
    "\n",
    "\n",
    "# 9. GridSearchCV LogisticRegression with class_weight\n",
    "\n",
    "param_grid = {\"C\": [0.01, 0.1, 1, 10, 100]}\n",
    "base_clf = LogisticRegression(\n",
    "    penalty=\"l2\",\n",
    "    class_weight=\"balanced\",\n",
    "    solver=\"saga\",\n",
    "    max_iter=5000,\n",
    "    random_state=42\n",
    ")\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "grid = GridSearchCV(\n",
    "    base_clf, param_grid,\n",
    "    scoring=\"neg_log_loss\",\n",
    "    cv=cv,\n",
    "    verbose=1,\n",
    "    n_jobs=-1\n",
    ")\n",
    "grid.fit(X_tr, y_tr)\n",
    "\n",
    "print(\"Best C:\", grid.best_params_[\"C\"])\n",
    "clf = grid.best_estimator_\n",
    "\n",
    "proba_va = clf.predict_proba(X_va)[:,1]\n",
    "print(\"Validation log-loss:\", log_loss(y_va, proba_va))\n",
    "\n",
    "# 10.  Predict on test set and save submission\n",
    "\n",
    "X_test = np.hstack([\n",
    "    pair_cosine_batch(test_pairs),\n",
    "    np.array([jaccard_auth(auth_map.get(u,\"\"), auth_map.get(v,\"\")) \n",
    "              for u,v in zip(test_pairs[\"src\"], test_pairs[\"dst\"])]).reshape(-1,1),\n",
    "    np.array([\n",
    "        len(list(nx.common_neighbors(Gu, u, v))) \n",
    "        for u,v in zip(test_pairs[\"src\"], test_pairs[\"dst\"])\n",
    "    ]).reshape(-1,1)\n",
    "])\n",
    "X_test = scaler.transform(X_test)\n",
    "proba_test = clf.predict_proba(X_test)[:,1]\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    \"ID\": np.arange(len(test_pairs)),\n",
    "    \"Label\": proba_test\n",
    "})\n",
    "submission.to_csv(\"submission_improved.csv\", index=False)\n",
    "print(\"Saved submission_improved.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46318fce-87e4-4ad1-8672-5185d53dbd30",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:mlproject]",
   "language": "python",
   "name": "conda-env-mlproject-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
