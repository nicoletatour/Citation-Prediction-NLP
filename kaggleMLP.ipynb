{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0af92e5-0c35-488a-b07d-0425814b543b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: 138499 abstracts; 138499 authors; 1091955 edges; 106692 test pairs\n",
      "end of load\n",
      "Common IDs: 100336 from 1091955 edges\n",
      "end of load\n",
      "end of build citation graph\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "7\n",
      "7.1\n",
      "Matching src IDs: 138480 of 2183910  | Matching dst IDs: 138499 of 2183910\n",
      "Samples[0:5] src,dst = [['0' '1']\n",
      " ['0' '2']\n",
      " ['1' '3']\n",
      " ['1' '5']\n",
      " ['1' '6']]\n",
      "Abstract IDs sample: ['0', '1', '2', '3', '4']\n",
      "5\n",
      "6\n",
      "6.1\n",
      "6.1\n",
      "7.2\n",
      "8\n",
      "9\n",
      "10\n",
      " entered mlp \n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "END of train epoch\n",
      "END of eval epoch\n",
      "END of train epoch\n",
      "END of eval epoch\n",
      "END of train epoch\n",
      "END of eval epoch\n",
      "END of train epoch\n",
      "END of eval epoch\n",
      "END of train epoch\n",
      "END of eval epoch\n",
      "END of train epoch\n",
      "END of eval epoch\n",
      "END of train epoch\n",
      "END of eval epoch\n",
      "END of train epoch\n",
      "END of eval epoch\n",
      "finished inside k forld for epoch\n",
      "Fold 0: best log-loss = 0.34963\n",
      "15\n",
      "16\n",
      "END of train epoch\n",
      "END of eval epoch\n",
      "END of train epoch\n",
      "END of eval epoch\n",
      "END of train epoch\n",
      "END of eval epoch\n",
      "END of train epoch\n",
      "END of eval epoch\n",
      "END of train epoch\n",
      "END of eval epoch\n",
      "END of train epoch\n",
      "END of eval epoch\n",
      "END of train epoch\n",
      "END of eval epoch\n",
      "END of train epoch\n",
      "END of eval epoch\n",
      "END of train epoch\n",
      "END of eval epoch\n",
      "END of train epoch\n",
      "END of eval epoch\n",
      "finished inside k forld for epoch\n",
      "Fold 1: best log-loss = 0.34913\n",
      "15\n",
      "16\n",
      "END of train epoch\n",
      "END of eval epoch\n",
      "END of train epoch\n",
      "END of eval epoch\n",
      "END of train epoch\n",
      "END of eval epoch\n",
      "END of train epoch\n",
      "END of eval epoch\n",
      "END of train epoch\n",
      "END of eval epoch\n",
      "END of train epoch\n",
      "END of eval epoch\n",
      "END of train epoch\n",
      "END of eval epoch\n",
      "END of train epoch\n",
      "END of eval epoch\n",
      "END of train epoch\n",
      "END of eval epoch\n",
      "END of train epoch\n",
      "END of eval epoch\n",
      "END of train epoch\n",
      "END of eval epoch\n",
      "finished inside k forld for epoch\n",
      "Fold 2: best log-loss = 0.34895\n",
      "15\n",
      "16\n",
      "END of train epoch\n",
      "END of eval epoch\n",
      "END of train epoch\n",
      "END of eval epoch\n",
      "END of train epoch\n",
      "END of eval epoch\n",
      "END of train epoch\n",
      "END of eval epoch\n",
      "END of train epoch\n",
      "END of eval epoch\n",
      "END of train epoch\n",
      "END of eval epoch\n",
      "END of train epoch\n",
      "END of eval epoch\n",
      "END of train epoch\n",
      "END of eval epoch\n",
      "END of train epoch\n",
      "END of eval epoch\n",
      "END of train epoch\n",
      "END of eval epoch\n",
      "finished inside k forld for epoch\n",
      "Fold 3: best log-loss = 0.35045\n",
      "15\n",
      "16\n",
      "END of train epoch\n",
      "END of eval epoch\n",
      "END of train epoch\n",
      "END of eval epoch\n",
      "END of train epoch\n",
      "END of eval epoch\n",
      "END of train epoch\n",
      "END of eval epoch\n",
      "END of train epoch\n",
      "END of eval epoch\n",
      "END of train epoch\n",
      "END of eval epoch\n",
      "END of train epoch\n",
      "END of eval epoch\n",
      "END of train epoch\n",
      "END of eval epoch\n",
      "finished inside k forld for epoch\n",
      "Fold 4: best log-loss = 0.34983\n",
      "Mean CV log-loss: 0.34960\n",
      "17\n",
      "18\n",
      "19\n",
      "END of train epoch\n",
      "END of train epoch\n",
      "END of train epoch\n",
      "END of train epoch\n",
      "END of train epoch\n",
      "END of train epoch\n",
      "END of train epoch\n",
      "END of train epoch\n",
      "END of train epoch\n",
      "END of train epoch\n",
      "END of train epoch\n",
      "END of train epoch\n",
      "END of train epoch\n",
      "END of train epoch\n",
      "END of train epoch\n",
      "END of train epoch\n",
      "END of train epoch\n",
      "END of train epoch\n",
      "END of train epoch\n",
      "END of train epoch\n",
      "END of train epoch\n",
      "END of train epoch\n",
      "END of train epoch\n",
      "END of train epoch\n",
      "END of train epoch\n",
      "END of train epoch\n",
      "END of train epoch\n",
      "END of train epoch\n",
      "END of train epoch\n",
      "END of train epoch\n",
      "Finished INSIDE FOR epoch train FINAL\n",
      "Full training finished – last epoch loss 0.34982\n",
      "5\n",
      "6\n",
      "6.1\n",
      "6.1\n",
      "20\n",
      "21\n",
      "   ID  probability\n",
      "0   0     0.877291\n",
      "1   1     0.240170\n",
      "2   2     0.137025\n",
      "3   3     0.240170\n",
      "4   4     0.143926\n",
      "Saved submission.csv – ready for Kaggle!\n"
     ]
    }
   ],
   "source": [
    "# 0. imports\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import log_loss\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "DATA_DIR = \"C:/Users/Dell/Desktop/nlp/kaggle/data_new/\"\n",
    "\n",
    "# 1. load two‐column text files \n",
    "\n",
    "def load_two_col_txt(path, col_names):\n",
    "    records = []\n",
    "    with open(path, encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        for line in f:\n",
    "            parts = line.rstrip(\"\\n\").split(\"|--|\", 1)\n",
    "            if len(parts) == 2:\n",
    "                records.append(parts)\n",
    "            else:\n",
    "                records.append([None, parts[0]])\n",
    "    return pd.DataFrame(records, columns=col_names)\n",
    "\n",
    "\n",
    "# 2.  Load abstracts, authors, citation edges, and test pairs\n",
    "\n",
    "abstracts = load_two_col_txt(\n",
    "    os.path.join(DATA_DIR, \"abstracts.txt\"),\n",
    "    [\"paper_id\", \"abstract\"]\n",
    ")\n",
    "authors   = load_two_col_txt(\n",
    "    os.path.join(DATA_DIR, \"authors.txt\"),\n",
    "    [\"paper_id\", \"authors\"]\n",
    ")\n",
    "\n",
    "edgelist = pd.read_csv(\n",
    "    os.path.join(DATA_DIR, \"edgelist.txt\"),\n",
    "    sep=\",\", names=[\"src\",\"dst\"], dtype=str, header=None\n",
    ")\n",
    "test_pairs = pd.read_csv(\n",
    "    os.path.join(DATA_DIR, \"test.txt\"),\n",
    "    sep=\",\", names=[\"src\",\"dst\"], dtype=str, header=None\n",
    ")\n",
    "\n",
    "\n",
    "for df in (abstracts, authors):\n",
    "    df[\"paper_id\"] = df[\"paper_id\"].str.strip()\n",
    "edgelist[\"src\"] = edgelist[\"src\"].str.strip()\n",
    "edgelist[\"dst\"] = edgelist[\"dst\"].str.strip()\n",
    "test_pairs[\"src\"] = test_pairs[\"src\"].str.strip()\n",
    "test_pairs[\"dst\"] = test_pairs[\"dst\"].str.strip()\n",
    "\n",
    "\n",
    "inter = set(edgelist[\"src\"]) & set(abstracts[\"paper_id\"])\n",
    "\n",
    "# 3.  Build directed citation graph\n",
    "\n",
    "G = nx.DiGraph()\n",
    "G.add_edges_from(edgelist.values)\n",
    "\n",
    "# 4. negative sampling and TF–IDF feature batching\n",
    "\n",
    "# tf-idf helper\n",
    "\n",
    "def tfidf_in_batches(texts, batch_size=100_000):\n",
    "    mats = []\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        chunk = texts[i:i+batch_size].fillna('')\n",
    "        mats.append(vectorizer.transform(chunk))\n",
    "    return mats\n",
    "\n",
    "# negative sampling \n",
    "\n",
    "def negative_sampling(G, n_neg):\n",
    "    nodes = np.array(list(G.nodes()))\n",
    "\n",
    "    k = n_neg * 3\n",
    "    us = np.random.choice(nodes, size=k, replace=True)\n",
    "    vs = np.random.choice(nodes, size=k, replace=True)\n",
    "\n",
    "    neg = []\n",
    "    seen = set()\n",
    "    for u, v in zip(us, vs):\n",
    "        if u == v or G.has_edge(u, v) or (u, v) in seen:\n",
    "            continue\n",
    "        seen.add((u, v))\n",
    "        neg.append((u, v))\n",
    "        if len(neg) >= n_neg:\n",
    "            break\n",
    "\n",
    "    return pd.DataFrame(neg, columns=['src','dst'])\n",
    "\n",
    "def build_samples(pos_edges, neg_ratio=1):\n",
    "    pos = pos_edges.copy(); pos['label'] = 1\n",
    "    n_neg = len(pos) * neg_ratio\n",
    "    neg = negative_sampling(G, int(n_neg))\n",
    "    neg['label'] = 0\n",
    "    # returns Dataframe with columns src, dst, label\n",
    "    return pd.concat([pos, neg], ignore_index=True)\n",
    "\n",
    "# jaccard helper\n",
    "\n",
    "def jaccard_auth(a: str, b: str):\n",
    "    sa, sb = set(a.split(',')), set(b.split(','))\n",
    "    return len(sa & sb) / len(sa | sb) if sa or sb else 0.0\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=5_000, stop_words='english')\n",
    "vectorizer.fit(abstracts['abstract'])\n",
    "\n",
    "# 5.  Fit TF–IDF vectorizer on all abstracts once\n",
    "\n",
    "def extract_features(df: pd.DataFrame):\n",
    "    merged = (\n",
    "        df.merge(abstracts, left_on='src', right_on='paper_id')\n",
    "          .merge(abstracts, left_on='dst', right_on='paper_id', suffixes=('_src', '_dst'))\n",
    "\n",
    "          .merge(authors,  left_on='src', right_on='paper_id')\n",
    "          .merge(authors,  left_on='dst', right_on='paper_id',\n",
    "                 suffixes=('_auth_src', '_auth_dst'))\n",
    "    )\n",
    "\n",
    "    #tf-idf cosine similarity\n",
    "\n",
    "    tf_src_chunks = tfidf_in_batches(merged['abstract_src'])\n",
    "    tf_dst_chunks = tfidf_in_batches(merged['abstract_dst'])\n",
    "    cosine_sim = np.concatenate([\n",
    "        (a.multiply(b)).sum(axis=1).A1\n",
    "        for a, b in zip(tf_src_chunks, tf_dst_chunks)\n",
    "    ])\n",
    "\n",
    "    #author jaccard\n",
    "\n",
    "    author_jac = [\n",
    "        jaccard_auth(a, b)\n",
    "        for a, b in zip(merged['authors_auth_src'],\n",
    "                        merged['authors_auth_dst'])\n",
    "    ]\n",
    "\n",
    "\n",
    "    return pd.DataFrame({\n",
    "        'cosine': cosine_sim,\n",
    "        'author_jac': author_jac,\n",
    "    })\n",
    "\n",
    "\n",
    "# 6.  Prepare training & validation data\n",
    "\n",
    "samples = build_samples(edgelist, neg_ratio=1)\n",
    "inter_src = set(samples[\"src\"]) & set(abstracts[\"paper_id\"])\n",
    "inter_dst = set(samples[\"dst\"]) & set(abstracts[\"paper_id\"])\n",
    "print(\"Matching src IDs:\", len(inter_src), \"of\", len(samples), \n",
    "      \" | Matching dst IDs:\", len(inter_dst), \"of\", len(samples))\n",
    "\n",
    "print(\"Samples[0:5] src,dst =\", samples[[\"src\",\"dst\"]].iloc[:5].values)\n",
    "print(\"Abstract IDs sample:\", list(abstracts[\"paper_id\"].iloc[:5]))\n",
    "X = extract_features(samples)\n",
    "\n",
    "y = samples['label']\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# 7.  PyTorch Dataset and DataLoader for batching\n",
    "\n",
    "class PairDataset(Dataset):\n",
    "    def __init__(self, X_np, y_np=None):\n",
    "        self.X = torch.tensor(X_np, dtype=torch.float32)\n",
    "        self.y = None\n",
    "        if y_np is not None:\n",
    "            self.y = torch.tensor(y_np, dtype=torch.float32).unsqueeze(1)\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "    def __getitem__(self, idx):\n",
    "        if self.y is None:\n",
    "            return self.X[idx]\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "# 8.  Define MLP model architecture\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# 9.  Training & evaluation functions\n",
    "\n",
    "def train_epoch(model, loader, loss_fn, optim, device):\n",
    "    model.train()\n",
    "    running = 0.0\n",
    "    for Xb, yb in loader:\n",
    "        Xb, yb = Xb.to(device), yb.to(device)\n",
    "        optim.zero_grad()\n",
    "        logits = model(Xb)\n",
    "        loss = loss_fn(logits, yb)\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        running += loss.item() * Xb.size(0)\n",
    "    return running / len(loader.dataset)\n",
    "\n",
    "def eval_epoch(model, loader, loss_fn, device):\n",
    "    model.eval()\n",
    "    running = 0.0\n",
    "    all_logits, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for Xb, yb in loader:\n",
    "            Xb, yb = Xb.to(device), yb.to(device)\n",
    "            logits = model(Xb)\n",
    "            loss = loss_fn(logits, yb)\n",
    "            running += loss.item() * Xb.size(0)\n",
    "            all_logits.append(logits.cpu())\n",
    "            all_labels.append(yb.cpu())\n",
    "            \n",
    "    avg_loss = running / len(loader.dataset)\n",
    "    probs = torch.sigmoid(torch.cat(all_logits)).numpy()\n",
    "    labels = torch.cat(all_labels).numpy()\n",
    "    sk_loss = log_loss(labels, probs)\n",
    "    return avg_loss, sk_loss\n",
    "\n",
    "# 10.  5‐fold Stratified Cross‐Validation with early stopping\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "cv_scores = []\n",
    "for fold, (tr_idx, val_idx) in enumerate(skf.split(X_scaled, y)):\n",
    "    X_tr, X_val = X_scaled[tr_idx], X_scaled[val_idx]\n",
    "    y_tr, y_val = y.iloc[tr_idx].values, y.iloc[val_idx].values\n",
    "   \n",
    "    train_ds = PairDataset(X_tr, y_tr)\n",
    "    val_ds   = PairDataset(X_val, y_val)\n",
    "    train_loader = DataLoader(train_ds, batch_size=1024, shuffle=True)\n",
    "    val_loader   = DataLoader(val_ds,   batch_size=2048, shuffle=False)\n",
    "    \n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    model = MLP(in_dim=X.shape[1]).to(device)\n",
    "    loss_fn = nn.BCEWithLogitsLoss()\n",
    "    optim   = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "    ()\n",
    "    best_sk = np.inf\n",
    "    patience, patience_cnt = 5, 0\n",
    "    for epoch in range(30):\n",
    "        train_epoch(model, train_loader, loss_fn, optim, device)\n",
    "        _, val_sk = eval_epoch(model, val_loader, loss_fn, device)\n",
    "        if val_sk < best_sk - 1e-4:\n",
    "            best_sk = val_sk\n",
    "            patience_cnt = 0\n",
    "        else:\n",
    "            patience_cnt += 1\n",
    "        if patience_cnt >= patience:\n",
    "            break\n",
    "\n",
    "    cv_scores.append(best_sk)\n",
    "    print(f\"Fold {fold}: best log-loss = {best_sk:.5f}\")\n",
    "\n",
    "print(f\"Mean CV log-loss: {np.mean(cv_scores):.5f}\")\n",
    "\n",
    "# 11.  Train final model on full dataset\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "full_ds = PairDataset(X_scaled, y.values)\n",
    "full_dl = DataLoader(full_ds, batch_size=1024, shuffle=True)\n",
    "\n",
    "final_model = MLP(X_scaled.shape[1]).to(device)\n",
    "crit = nn.BCEWithLogitsLoss()\n",
    "opt  = torch.optim.Adam(final_model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "\n",
    "for epoch in range(30):\n",
    "    loss = train_epoch(final_model, full_dl, crit, opt, device)\n",
    "print(f\"Full training finished – last epoch loss {loss:.5f}\")\n",
    "\n",
    "# 12.  Prepare test features, predict, and save submission\n",
    "\n",
    "X_test_df = extract_features(test_pairs)\n",
    "X_test    = scaler.transform(X_test_df)\n",
    "test_ds = PairDataset(X_test)\n",
    "test_dl = DataLoader(test_ds, batch_size=2048, shuffle=False)\n",
    "final_model.eval()\n",
    "logits = []\n",
    "with torch.no_grad():\n",
    "    for xb in test_dl:\n",
    "        logits.append(final_model(xb.to(device)).cpu())\n",
    "\n",
    "probs = torch.sigmoid(torch.cat(logits)).squeeze(1).numpy()  \n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    'ID': test_pairs.index,      \n",
    "    'probability': probs         \n",
    "})\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "print(submission.head())\n",
    "print('Saved submission.csv ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21a8051-47d9-4cd6-b431-fe63fdd11188",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
