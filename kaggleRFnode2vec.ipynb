{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc402b6e-103b-4896-972c-7625433a8044",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fb17147aebc4d7cabd3bebcef0201ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing transition probabilities:   0%|          | 0/138499 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating walks (CPU: 2):  26%|██▌       | 13/50 [28:09<1:24:07, 136.42s/it]"
     ]
    }
   ],
   "source": [
    "# 0.Ιmports\n",
    "import os, warnings, multiprocessing as mp\n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from node2vec import Node2Vec\n",
    "\n",
    "# 1.  Helper to load two‐column text files split by \"|--|\"\n",
    "\n",
    "def load_two_col_txt(path, cols):\n",
    "    rec = []\n",
    "    with open(path, encoding='utf-8', errors='ignore') as f:\n",
    "        for ln in f:\n",
    "            if '|--|' in ln:\n",
    "                pid, txt = ln.rstrip('\\n').split('|--|', 1)\n",
    "            else:                      \n",
    "                pid, txt = None, ln.rstrip('\\n')\n",
    "            rec.append([pid, txt])\n",
    "    return pd.DataFrame(rec, columns=cols)\n",
    "\n",
    "# 2.  Load data\n",
    "\n",
    "DATA_DIR = \"/Users/nicoletatour/Desktop/nlp/kaggle/data_new/\"\n",
    "\n",
    "abstracts = load_two_col_txt(os.path.join(DATA_DIR, 'abstracts.txt'),\n",
    "                             ['paper_id', 'abstract'])\n",
    "authors   = load_two_col_txt(os.path.join(DATA_DIR, 'authors.txt'),\n",
    "                             ['paper_id', 'authors'])\n",
    "edges     = pd.read_csv(os.path.join(DATA_DIR, 'edgelist.txt'),\n",
    "                        names=['src', 'dst'], dtype=str, header=None)\n",
    "test_pairs= pd.read_csv(os.path.join(DATA_DIR, 'test.txt'),\n",
    "                        names=['src', 'dst'], dtype=str, header=None)\n",
    "# clean whitespace from all IDs\n",
    "for df in (abstracts, authors):\n",
    "    df['paper_id'] = df['paper_id'].str.strip()\n",
    "edges['src'] = edges['src'].str.strip(); edges['dst']=edges['dst'].str.strip()\n",
    "test_pairs['src']=test_pairs['src'].str.strip(); test_pairs['dst']=test_pairs['dst'].str.strip()\n",
    "\n",
    "# 3.  Graph‐of‐Words features: PageRank sum & clustering average\n",
    "\n",
    "def simple_tok(t): \n",
    "    return [w for w in t.lower().split() if w.isalpha()]\n",
    "\n",
    "WINDOW = 3 # window size\n",
    "\n",
    "pr_sum, cc_avg = {}, {}\n",
    "for _, row in abstracts.iterrows():\n",
    "    pid, txt = row['paper_id'], row['abstract'] or ''\n",
    "    toks = simple_tok(txt)\n",
    "    if not toks:\n",
    "        pr_sum[pid] = cc_avg[pid] = 0.0\n",
    "        continue\n",
    "    Gt = nx.Graph()\n",
    "    Gt.add_nodes_from(toks)\n",
    "    for i in range(len(toks)):\n",
    "        for j in range(i+1, min(i+WINDOW, len(toks))):\n",
    "            if toks[i]!=toks[j]:\n",
    "                Gt.add_edge(toks[i], toks[j])\n",
    "    pr_sum[pid] = sum(nx.pagerank(Gt, alpha=.85).values())\n",
    "    cc_avg[pid] = np.mean(list(nx.clustering(Gt).values()))\n",
    "\n",
    "\n",
    "# 4. Build Citation graph & Node2Vec\n",
    "Gu = nx.Graph()\n",
    "Gu.add_edges_from(edges.values)     \n",
    "\n",
    "\n",
    "n2v = Node2Vec(Gu, dimensions=64, walk_length=30, num_walks=200,\n",
    "               workers=mp.cpu_count(), seed=42)\n",
    "n2v_model = n2v.fit(window=10, min_count=1, batch_words=4)\n",
    "\n",
    "dim = n2v_model.wv.vector_size\n",
    "zero_vec = np.zeros(dim, dtype=np.float32)\n",
    "emb = {str(n): n2v_model.wv[str(n)] for n in Gu.nodes()}\n",
    "\n",
    "# Cosine similarity between node2vec embeddings of u and v\n",
    "def n2v_cos(u, v):\n",
    "    vu, vv = emb.get(u, zero_vec), emb.get(v, zero_vec)\n",
    "    if (vu is zero_vec) or (vv is zero_vec): return 0.0\n",
    "    return float(np.dot(vu, vv) /\n",
    "                 (np.linalg.norm(vu)*np.linalg.norm(vv) + 1e-8))\n",
    "\n",
    "# 5.  Balanced negative sampling (degree‐aware)\n",
    "\n",
    "edges['label']=1\n",
    "nodes = np.array(list(Gu.nodes()))\n",
    "deg   = np.array([Gu.degree(n) for n in nodes], dtype=float)\n",
    "deg_p = deg / deg.sum()\n",
    "\n",
    "# Set negative sampling ratio\n",
    "NEG_RATIO = 0.25\n",
    "n_neg = int(len(edges)*NEG_RATIO)\n",
    "neg = pd.DataFrame({\n",
    "        'src': np.random.choice(nodes, n_neg, p=deg_p),\n",
    "        'dst': np.random.choice(nodes, n_neg, p=deg_p)})\n",
    "neg = neg[~neg.apply(tuple,1).isin(edges.apply(tuple,1))]\n",
    "neg['label']=0\n",
    "\n",
    "samples = pd.concat([edges, neg], ignore_index=True)\n",
    "\n",
    "# 6.  Feature extraction: TF–IDF cosine, Author‐Jaccard, Common neighbors\n",
    "\n",
    "tfv = TfidfVectorizer(max_features=10000, stop_words='english')\n",
    "tfv.fit(abstracts['abstract'].fillna(''))\n",
    "A = tfv.transform(abstracts['abstract'].fillna(''))\n",
    "pid2idx = {p:i for i,p in enumerate(abstracts['paper_id'])}\n",
    "\n",
    "# TF-IDF\n",
    "def tfidf_cos(u,v):\n",
    "    iu,pv = pid2idx.get(u,-1), pid2idx.get(v,-1)\n",
    "    return (A[iu].multiply(A[pv])).sum() if iu>=0 and pv>=0 else 0.0\n",
    "\n",
    "# Jaccard similarity\n",
    "auth = dict(zip(authors['paper_id'], authors['authors']))\n",
    "def jac(a,b):\n",
    "    sa, sb = set(a.split(',')), set(b.split(','))\n",
    "    return 0. if not (sa or sb) else len(sa&sb)/len(sa|sb)\n",
    "\n",
    "# common neighbors\n",
    "def com_nei(u,v):\n",
    "    try: return len(list(nx.common_neighbors(Gu,u,v)))\n",
    "    except: return 0\n",
    "\n",
    "# vectorize feature computations\n",
    "X_cos     = np.array([tfidf_cos(u,v) for u,v in zip(samples.src, samples.dst)]).reshape(-1,1)\n",
    "X_jac     = np.array([jac(auth.get(u,''),auth.get(v,'')) for u,v in zip(samples.src,samples.dst)]).reshape(-1,1)\n",
    "X_comm    = np.array([com_nei(u,v)   for u,v in zip(samples.src, samples.dst)]).reshape(-1,1)\n",
    "X_pr      = np.array([pr_sum.get(u,0)+pr_sum.get(v,0) for u,v in zip(samples.src,samples.dst)]).reshape(-1,1)\n",
    "X_cc      = np.array([cc_avg.get(u,0)+cc_avg.get(v,0) for u,v in zip(samples.src,samples.dst)]).reshape(-1,1)\n",
    "X_n2v     = np.array([n2v_cos(u,v)   for u,v in zip(samples.src, samples.dst)]).reshape(-1,1)\n",
    "\n",
    "X_all = np.hstack([X_cos, X_jac, X_comm, X_pr, X_cc, X_n2v])\n",
    "y_all = samples['label'].values\n",
    "print(\"Feature matrix:\", X_all.shape)\n",
    "\n",
    "# 7.  Train / validation split\n",
    "X_tr, X_va, y_tr, y_va = train_test_split(\n",
    "        X_all, y_all, test_size=0.2, stratify=y_all, random_state=42)\n",
    "scaler = StandardScaler(with_mean=False)\n",
    "X_tr = scaler.fit_transform(X_tr);  X_va = scaler.transform(X_va)\n",
    "\n",
    "# 8.  RandomForest with class‐weights\n",
    "pos_w = (len(y_all)-y_all.sum())/y_all.sum()\n",
    "rf = RandomForestClassifier(\n",
    "        n_estimators=200, max_depth=20, min_samples_leaf=2,\n",
    "        class_weight={0:1.0, 1:pos_w}, random_state=42, n_jobs=-1)\n",
    "rf.fit(X_tr, y_tr)\n",
    "\n",
    "print(\"Validation log-loss:\", log_loss(y_va, rf.predict_proba(X_va)[:,1]))\n",
    "\n",
    "# 9. submission for test set\n",
    "xt_cos  = np.array([tfidf_cos(u,v) for u,v in zip(test_pairs.src,test_pairs.dst)]).reshape(-1,1)\n",
    "xt_jac  = np.array([jac(auth.get(u,''),auth.get(v,'')) for u,v in zip(test_pairs.src,test_pairs.dst)]).reshape(-1,1)\n",
    "xt_com  = np.array([com_nei(u,v) for u,v in zip(test_pairs.src,test_pairs.dst)]).reshape(-1,1)\n",
    "xt_pr   = np.array([pr_sum.get(u,0)+pr_sum.get(v,0) for u,v in zip(test_pairs.src,test_pairs.dst)]).reshape(-1,1)\n",
    "xt_cc   = np.array([cc_avg.get(u,0)+cc_avg.get(v,0) for u,v in zip(test_pairs.src,test_pairs.dst)]).reshape(-1,1)\n",
    "xt_n2v  = np.array([n2v_cos(u,v) for u,v in zip(test_pairs.src,test_pairs.dst)]).reshape(-1,1)\n",
    "\n",
    "X_test = scaler.transform(np.hstack([xt_cos, xt_jac, xt_com, xt_pr, xt_cc, xt_n2v]))\n",
    "probs  = rf.predict_proba(X_test)[:,1]\n",
    "\n",
    "pd.DataFrame({\"ID\":test_pairs.index, \"Label\":probs}) \\\n",
    "  .to_csv(\"submission_node2vec_rf.csv\", index=False)\n",
    "print(\"submission_node2vec_rf.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e282e1-b611-4bd2-97f7-a9ebe904f2e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
