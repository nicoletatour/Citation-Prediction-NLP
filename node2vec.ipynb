{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fdb32ea-47ea-4887-a7b3-9ed38cfce9ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import os, warnings, multiprocessing as mp\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47ea066-e92a-4139-a3e0-ba74d39cee1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load\n",
    "def load_two_col_txt(path, cols):\n",
    "    rec = []\n",
    "    with open(path, encoding='utf-8', errors='ignore') as f:\n",
    "        for ln in f:\n",
    "            if '|--|' in ln:\n",
    "                pid, txt = ln.rstrip('\\n').split('|--|', 1)\n",
    "            else:                      # fallback\n",
    "                pid, txt = None, ln.rstrip('\\n')\n",
    "            rec.append([pid, txt])\n",
    "    return pd.DataFrame(rec, columns=cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554ad5bf-2c4e-4acd-a5e6-87d981d3f87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#path\n",
    "DATA_DIR = \"C:/Users/Dell/Desktop/NLP/kaggle/data_new/\"\n",
    "\n",
    "#files\n",
    "abstracts = load_two_col_txt(os.path.join(DATA_DIR, 'abstracts.txt'),\n",
    "                             ['paper_id', 'abstract'])\n",
    "authors   = load_two_col_txt(os.path.join(DATA_DIR, 'authors.txt'),\n",
    "                             ['paper_id', 'authors'])\n",
    "edges     = pd.read_csv(os.path.join(DATA_DIR, 'edgelist.txt'),\n",
    "                        names=['src', 'dst'], dtype=str, header=None)\n",
    "test_pairs= pd.read_csv(os.path.join(DATA_DIR, 'test.txt'),\n",
    "                        names=['src', 'dst'], dtype=str, header=None)\n",
    "#removes spaces\n",
    "for df in (abstracts, authors):\n",
    "    df['paper_id'] = df['paper_id'].str.strip()\n",
    "edges['src'] = edges['src'].str.strip(); edges['dst']=edges['dst'].str.strip()\n",
    "test_pairs['src']=test_pairs['src'].str.strip(); test_pairs['dst']=test_pairs['dst'].str.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73caa077-1656-4b6b-8b80-1805868686aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def simple_tok(t): return [w for w in t.lower().split() if w.isalpha()]\n",
    "WINDOW = 3  # size of the sliding-window for co-occurrence edges\n",
    "\n",
    "pr_sum, cc_avg = {}, {}\n",
    "for _, row in abstracts.iterrows():\n",
    "    pid, txt = row['paper_id'], row['abstract'] or ''\n",
    "    # Tokenization\n",
    "    toks = simple_tok(txt)\n",
    "    # If the abstract has no valid tokens, store zeros and skip the rest\n",
    "    if not toks:\n",
    "        pr_sum[pid] = cc_avg[pid] = 0.0\n",
    "        continue\n",
    "    # Build co-occurrence graph\n",
    "    Gt = nx.Graph()\n",
    "    Gt.add_nodes_from(toks)\n",
    "    for i in range(len(toks)):\n",
    "        for j in range(i+1, min(i+WINDOW, len(toks))):\n",
    "            if toks[i]!=toks[j]:\n",
    "                Gt.add_edge(toks[i], toks[j])\n",
    "    pr_sum[pid] = sum(nx.pagerank(Gt, alpha=.85).values())\n",
    "    cc_avg[pid] = np.mean(list(nx.clustering(Gt).values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d1eaad-d03e-44f1-bbe8-3c4b2c0b7214",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build an undirected graph from the edge list\n",
    "Gu = nx.Graph()\n",
    "Gu.add_edges_from(edges.values)     # μη κατευθυνόμενο γρ. για Node2Vec\n",
    "\n",
    "# Train a Node2Vec model to obtain low-dimensional node embeddings\n",
    "from node2vec import Node2Vec\n",
    "n2v = Node2Vec(Gu, dimensions=32, walk_length=20, num_walks=20,\n",
    "               workers= 1 , seed=42, quiet=True)\n",
    "n2v_model = n2v.fit(window=10, min_count=1, batch_words=4)\n",
    "\n",
    "# Collect the trained embeddings into a convenient dictionary\n",
    "dim = n2v_model.wv.vector_size\n",
    "zero_vec = np.zeros(dim, dtype=np.float32)\n",
    "emb = {str(n): n2v_model.wv[str(n)] for n in Gu.nodes()}\n",
    "\n",
    "# cosine similarity between two node embeddings\n",
    "def n2v_cos(u, v):\n",
    "    vu, vv = emb.get(u, zero_vec), emb.get(v, zero_vec)\n",
    "    if (vu is zero_vec) or (vv is zero_vec): return 0.0\n",
    "    return float(np.dot(vu, vv) /\n",
    "                 (np.linalg.norm(vu)*np.linalg.norm(vv) + 1e-8))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d407d5-b128-4666-b10e-dd4c23d46536",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label the observed edges as positive\n",
    "edges['label']=1\n",
    "nodes = np.array(list(Gu.nodes()))\n",
    "deg   = np.array([Gu.degree(n) for n in nodes], dtype=float)\n",
    "deg_p = deg / deg.sum()\n",
    "\n",
    "# 1 negative for every 4 positives\n",
    "NEG_RATIO = 0.25\n",
    "n_neg = int(len(edges)*NEG_RATIO)\n",
    "neg = pd.DataFrame({\n",
    "        'src': np.random.choice(nodes, n_neg, p=deg_p),\n",
    "        'dst': np.random.choice(nodes, n_neg, p=deg_p)})\n",
    "# # Remove pairs that actually exist in the real edge list\n",
    "neg = neg[~neg.apply(tuple,1).isin(edges.apply(tuple,1))]\n",
    "neg['label']=0\n",
    "\n",
    "# Merge positive and negative samples\n",
    "samples = pd.concat([edges, neg], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ffc83a-ac85-421d-b53f-a0a2904e496d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfv = TfidfVectorizer(max_features=10000, stop_words='english')\n",
    "tfv.fit(abstracts['abstract'].fillna(''))\n",
    "A = tfv.transform(abstracts['abstract'].fillna(''))\n",
    "pid2idx = {p:i for i,p in enumerate(abstracts['paper_id'])}\n",
    "\n",
    "# Cosine-like similarity between papers u and v using their TF-ID vectors \n",
    "def tfidf_cos(u,v):\n",
    "    iu,pv = pid2idx.get(u,-1), pid2idx.get(v,-1)\n",
    "    return (A[iu].multiply(A[pv])).sum() if iu>=0 and pv>=0 else 0.0\n",
    "\n",
    "# Author-based similarity: Jaccard on author lists\n",
    "auth = dict(zip(authors['paper_id'], authors['authors']))\n",
    "def jac(a,b):\n",
    "    sa, sb = set(a.split(',')), set(b.split(','))\n",
    "    return 0. if not (sa or sb) else len(sa&sb)/len(sa|sb)\n",
    "\n",
    "# Topology-based similarity: number of common neighbors\n",
    "def com_nei(u,v):\n",
    "    try: return len(list(nx.common_neighbors(Gu,u,v)))\n",
    "    except: return 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3feb1983-8af4-42a4-a89a-340a556ef621",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute each feature column (N pairs x 1) and keep as 2-D array\n",
    "X_cos     = np.array([tfidf_cos(u,v) for u,v in zip(samples.src, samples.dst)]).reshape(-1,1)\n",
    "X_jac     = np.array([jac(auth.get(u,''),auth.get(v,'')) for u,v in zip(samples.src,samples.dst)]).reshape(-1,1)\n",
    "X_comm    = np.array([com_nei(u,v)   for u,v in zip(samples.src, samples.dst)]).reshape(-1,1)\n",
    "X_pr      = np.array([pr_sum.get(u,0)+pr_sum.get(v,0) for u,v in zip(samples.src,samples.dst)]).reshape(-1,1)\n",
    "X_cc      = np.array([cc_avg.get(u,0)+cc_avg.get(v,0) for u,v in zip(samples.src,samples.dst)]).reshape(-1,1)\n",
    "X_n2v     = np.array([n2v_cos(u,v)   for u,v in zip(samples.src, samples.dst)]).reshape(-1,1)\n",
    "\n",
    "X_all = np.hstack([X_cos, X_jac, X_comm, X_pr, X_cc, X_n2v])\n",
    "y_all = samples['label'].values\n",
    "print(\"Feature matrix:\", X_all.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7481dc8e-fdad-4c0d-957b-db4befdf06ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr, X_va, y_tr, y_va = train_test_split(\n",
    "        X_all, y_all, test_size=0.2, stratify=y_all, random_state=42)\n",
    "scaler = StandardScaler(with_mean=False)\n",
    "X_tr = scaler.fit_transform(X_tr);  X_va = scaler.transform(X_va)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58630436-ce3d-4954-b60f-d024f1fc29ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle class imbalance with explicit weights\n",
    "pos_w = (len(y_all) - y_all.sum()) / y_all.sum()  \n",
    "\n",
    "# Hyper-parameter grid for RandomForest\n",
    "param_grid = {\n",
    "    \"n_estimators\":    [200],     \n",
    "    \"max_depth\":       [10, None],\n",
    "    \"min_samples_leaf\":[2],           \n",
    "}\n",
    "\n",
    "# Base RandomForest model (class-weighted)\n",
    "base_rf = RandomForestClassifier(\n",
    "            class_weight={0:1.0, 1:pos_w},\n",
    "            random_state=42,\n",
    "            n_jobs=-1)\n",
    "\n",
    "# Stratified K-fold CV: preserves class ratio in each split\n",
    "cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "\n",
    "# Grid-search with negative log-loss (lower = better)\n",
    "grid = GridSearchCV(\n",
    "        estimator   = base_rf,\n",
    "        param_grid  = param_grid,\n",
    "        scoring     = \"neg_log_loss\",\n",
    "        cv          = cv,\n",
    "        verbose     = 1,\n",
    "        n_jobs      = -1)\n",
    "\n",
    "grid.fit(X_tr, y_tr)\n",
    "\n",
    "# Retrieve the best model & evaluate on held-out validation data\n",
    "print(\"Best RF params:\", grid.best_params_)\n",
    "rf_best = grid.best_estimator_\n",
    "\n",
    "print(\"Hold-out log-loss:\",\n",
    "      log_loss(y_va, rf_best.predict_proba(X_va)[:, 1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d46a9ba6-846e-400b-849e-959d518fa9b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build every feature column for the test link pairs\n",
    "xt_cos  = np.array([tfidf_cos(u,v) for u,v in zip(test_pairs.src,test_pairs.dst)]).reshape(-1,1)\n",
    "xt_jac  = np.array([jac(auth.get(u,''),auth.get(v,'')) for u,v in zip(test_pairs.src,test_pairs.dst)]).reshape(-1,1)\n",
    "xt_com  = np.array([com_nei(u,v) for u,v in zip(test_pairs.src,test_pairs.dst)]).reshape(-1,1)\n",
    "xt_pr   = np.array([pr_sum.get(u,0)+pr_sum.get(v,0) for u,v in zip(test_pairs.src,test_pairs.dst)]).reshape(-1,1)\n",
    "xt_cc   = np.array([cc_avg.get(u,0)+cc_avg.get(v,0) for u,v in zip(test_pairs.src,test_pairs.dst)]).reshape(-1,1)\n",
    "xt_n2v  = np.array([n2v_cos(u,v) for u,v in zip(test_pairs.src,test_pairs.dst)]).reshape(-1,1)\n",
    "\n",
    "# Concatenate the six columns, then scale with the SAME scaler that was fitted on the training features\n",
    "X_test = scaler.transform(\n",
    "            np.hstack([xt_cos, xt_jac, xt_com, xt_pr, xt_cc, xt_n2v])\n",
    "         )\n",
    "\n",
    "# Predict link probabilities with the tuned RandomForest\n",
    "probs = rf_best.predict_proba(X_test)[:, 1]      \n",
    "\n",
    "pd.DataFrame({\n",
    "        \"ID\":    test_pairs.index,\n",
    "        \"Label\": probs\n",
    "}).to_csv(\"submission_node2vec_rf.csv\", index=False)\n",
    "\n",
    "print(\"Saved submission_node2vec_rf.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b861e757-bb13-4a0e-9f36-886926767166",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
